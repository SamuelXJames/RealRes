{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQumtCm7/Xm/QkT9dtD+Vf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelXJames/RealRes/blob/main/models/build_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "KO2rL0YOgb_Z",
        "outputId": "0b2d7718-23b8-41d8-8a68-b6e3600b2c45"
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "from datetime import timedelta\n",
        "from tqdm import tqdm\n",
        "from generator import Build_generator\n",
        "from descriminator import Build_descriminator\n",
        "from load_vgg19 import Load_VGG19\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class Build_Model:\n",
        "\n",
        "  def __init__(self,patch_size,log_dir):\n",
        "    self.log_dir = log_dir\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "    gen = Build_generator(patch_size)\n",
        "    dec = Build_descriminator(patch_size)\n",
        "    vgg = Load_VGG19(patch_size)\n",
        "\n",
        "    # Generator/Descriminator/VGG19\n",
        "    self.g = gen.build_generator()\n",
        "    self.d = dec.build_descriminator()\n",
        "    self.v = veg.load_model()\n",
        "\n",
        "    self.d.model.trainable = False\n",
        "    self.v.mode.trainable = False\n",
        "\n",
        "    # Metrics\n",
        "\n",
        "  def build_model(self):\n",
        "\n",
        "    # Generator Prediction\n",
        "    LR = Input(shape=(self.lr_patch_size,) * 2 + (3,))\n",
        "    SR = self.g.model(LR)\n",
        "    outputs = [SR]\n",
        "    \n",
        "    # Generator Losses \n",
        "    losses = ['mae'] \n",
        "    loss_weights = [1.0]\n",
        "\n",
        "    # Descriminator Losses\n",
        "    d_pred = d.model(SR)\n",
        "    outputs.append(d_pred)\n",
        "\n",
        "    # Descriminator Losses\n",
        "    losses.append(['binary_crossentropy'])\n",
        "    loss_weights.append(0.003)\n",
        "\n",
        "    # VGG19 Deep Feature Extraction\n",
        "    v_features = v.model(SR)\n",
        "    outputs.extend([*v_features]) #not sure if *expression is necessary here\n",
        "\n",
        "    # VGG19 \n",
        "    losses.extend(['mse'] * len(sr_feats))\n",
        "    loss_weights.extend([(1/12) / len(sr_feats)] * len(sr_feats)\n",
        "\n",
        "    GAN = Model(inputs = SR, outputs = outputs)\n",
        "\n",
        "    optimizer = Adam(beta_1 = 0.9, beta_2 = 0.999, \n",
        "                     lr = 0.0004, epsilon = None)\n",
        "    GAN.compile(loss = losses, loss_weights = loss_weights,\n",
        "                optimizer = optiizer, metrics = self.metrics)\n",
        "    \n",
        "    return GAN\n",
        "\n",
        "  def train_model(self,model, epochs, steps_per_epoch, batch_size, metrics):\n",
        "\n",
        "    # Start tensorboard\n",
        "    tensorboard = Tensorboard(log_dir = self.log_dir)\n",
        "    tensorboard.set_model(model)\n",
        "\n",
        "    # Get Validation Data\n",
        "    [HR_data,LR_data] = self.get_data()\n",
        "    y_valid = [HR_data]\n",
        "   \n",
        "    d_shape = list(self.d.model.outputs[0].shape)[1:4]\n",
        "    real = np.ones([batch_size] + d_shape)\n",
        "    fake = np.zeros([batch_size] + d_shape)\n",
        "\n",
        "    valid_d = np.ones([len(HR_data)] + d_shape)\n",
        "    y_valid.append(valid_d)\n",
        "    \n",
        "    valid_v = self.v.mode.predict(HR_data)\n",
        "    y_valid.extend([*valid_v])\n",
        "\n",
        "    for epoch in range (starting_epoch, epochs):\n",
        "      K.set_value(model.optimizer.lr, self._lr_scheduler(epoch=epoch))\n",
        "\n",
        "      flatness = self.flatness_schedue(epoch)\n",
        "\n",
        "      start_time = time()\n",
        "      for step in tqdm(range(steps_per_epoch)):\n",
        "        [HR_batch,LR_batch] = self.data_handler.get_batch(batch_size,flatness=flatness)\n",
        "        y_train = HR_batch\n",
        "        training_losses = {}\n",
        "\n",
        "        # Train Descriminator\n",
        "        sr = self.g.model.predict(LR_batch)\n",
        "        d_loss_real = self.discriminator.model.train_on_batch(LR_batch,real)\n",
        "        d_loss_fake = self.discriminator.model.train_on_batch(sr, fake)\n",
        "\n",
        "        # FOrmat Training Losses for Tensorboard\n",
        "        d_loss_fake = self._format_losses('train_d_fake_', d_loss_fake, self.discriminator.model.metrics_name)\n",
        "        d_loss_real = self._format_losses('train_d_real_', d_loss_real, self.discriminator.model.metrics_names)\n",
        "        \n",
        "        training_losses.update(d_loss_real)\n",
        "        training_losses.update(d_loss_fake)\n",
        "        y_train.append(valid)\n",
        "\n",
        "        # Train Generator\n",
        "        HR_features = v.model.predict(HR_batch)\n",
        "        y_train.extend([*HR_features])\n",
        "        model_losses = self.model.train_on_batch(batch['lr'], y_train)\n",
        "        training_losses.update(model_losses)\n",
        "\n",
        "        model_losses = self._format_losses('train_', model_losses, self.model.metrics_names)\n",
        "      \n",
        "      time_per_epoch = time() - start_time\n",
        "      \n",
        "      validation_losses = self.model.evaluate(LR_data, y_validation, batch_size=batch_size)\n",
        "      validation_losses = self._format_losses( 'val_', validation_losses, self.model.metrics_names)\n",
        "\n",
        "      end_losses = {}\n",
        "      end_losses.update(validation_losses)\n",
        "      end_losses.update(training_losses)\n",
        "\n",
        "      self.helper.on_epoch_end()\n",
        "    self.tensorboard.on_epoch_end(epoch, validation_losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "                     \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-aa457cf66ee9>\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLTUHD7TIiIi",
        "outputId": "24daf36e-9081-49cf-b6ed-b49cc42072a3"
      },
      "source": [
        "test = [1,2,3]\n",
        "t = [[3,4],[6]]\n",
        "test.extend([*t])\n",
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, [3, 4], [6]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}