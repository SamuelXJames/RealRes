# -*- coding: utf-8 -*-
"""
Created on Sun Nov  8 21:00:36 2020

@author: S
"""

import torch
import torch.nn as nn
from torch.autograd import Variable
import time as t
import matplotlib.pyplot as plt
plt.switch_backend('agg')
import os
from utils.tensorboard_logger import Logger
from torchvision import utils
import numpy as np
import tensorflow as tf


class Generator(torch.nn.Module):
    def __init__(self, channels):
        super().__init__()
        # Filters [1024, 512, 256]
        # Input_dim = 100
        # Output_dim = C (number of channels)
        self.main_module = nn.Sequential(
            # Z latent vector 100
            nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0),
            nn.BatchNorm2d(num_features=1024),
            nn.ReLU(True),

            # State (1024x4x4)
            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(num_features=512),
            nn.ReLU(True),

            # State (512x8x8)
            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(num_features=256),
            nn.ReLU(True),

            # State (256x16x16)
            nn.ConvTranspose2d(in_channels=256, out_channels=1, kernel_size=4, stride=2, padding=1),
            # output of main module --> Image (Cx25x25)
            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=8, stride=1, padding=0))

        self.output = nn.Tanh()


    def forward(self, x):
        x = self.main_module(x)
        return self.output(x)

class Discriminator(torch.nn.Module):
    def __init__(self, channels):
        super().__init__()
        # Filters [256, 512, 1024]
        # Input_dim = channels (Cx64x64)
        # Output_dim = 1
        self.main_module = nn.Sequential(
            # Omitting batch normalization in critic because our new penalized training objective (WGAN with gradient penalty) is no longer valid
            # in this setting, since we penalize the norm of the critic's gradient with respect to each input independently and not the enitre batch.
            # There is not good & fast implementation of layer normalization --> using per instance normalization nn.InstanceNorm2d()
            # Image (Cx25x25)
            nn.Conv2d(in_channels=1, out_channels=256, kernel_size=4, stride=2, padding=1),
            nn.InstanceNorm2d(256, affine=True),
            nn.LeakyReLU(0.2, inplace=True),

            # State (256x12x12)
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),
            nn.InstanceNorm2d(512, affine=True),
            nn.LeakyReLU(0.2, inplace=True),

            # State (512x6x6)
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),
            nn.InstanceNorm2d(1024, affine=True),
            nn.LeakyReLU(0.2, inplace=True))
            # output of main module --> State (1024x4x4)

        self.output = nn.Sequential(
            # The output of D is no longer a probability, we do not apply sigmoid at the output of D.
            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=3, stride=1, padding=0))


    def forward(self, x):
        x = self.main_module(x)
        return self.output(x)

    def feature_extraction(self, x):
        # Use discriminator for feature extraction then flatten to vector of 16384
        x = self.main_module(x)
        return x.view(-1, 1024*4*4)


class WGAN_CP(object):
    def __init__(self, save_path,channels=1, generator_iters=400000):

        print("WGAN_CP init model.")

        # Folders to save output
        self.MODEL_PATH = save_path["MODEL_PATH"]
        self.IMG_OUT = save_path["IMG_OUT"]
        self.INT_IMG = save_path["INT_IMG"]
        
        
        self.G = Generator(channels)
        self.D = Discriminator(channels)
        self.C = 1

        # check if cuda is available
        self.check_cuda(True)

        # WGAN values from paper
        self.learning_rate = 0.00005

        self.batch_size = 128
        self.weight_cliping_limit = 0.01

        # WGAN with gradient clipping uses RMSprop instead of ADAM
        self.d_optimizer = torch.optim.RMSprop(self.D.parameters(), lr=self.learning_rate)
        self.g_optimizer = torch.optim.RMSprop(self.G.parameters(), lr=self.learning_rate)

        self.number_of_images = 10

        self.generator_iters = generator_iters
        self.critic_iter = 5

        # Load saved model
        if len(os.listdir(self.MODEL_PATH)) != 0:
          G_model_filename = 'generator.pkl'
          D_model_filename = 'discriminator.pkl'
          self.load_model(D_model_filename, G_model_filename)
        
        

    def check_cuda(self, cuda_flag=False):
        if cuda_flag:
            self.cuda_index = 0
            self.cuda = True
            self.D.cuda()
            self.G.cuda()
            print("Cuda enabled flag: {}".format(self.cuda))


    def train(self, train_loader):
        self.t_begin = t.time()
        #self.file = open("inception_score_graph.txt", "w")

        # Now batches are callable self.data.next()
        self.data = self.get_infinite_batches(train_loader)

        one = torch.FloatTensor([1])
        mone = one * -1
        if self.cuda:
            one = one.cuda()
            mone = mone.cuda()    

        for g_iter in range(self.generator_iters):
            # Requires grad, Generator requires_grad = False
            for p in self.D.parameters():
                p.requires_grad = True

            # Train Dicriminator forward-loss-backward-update self.critic_iter times while 1 Generator forward-loss-backward-update
            for d_iter in range(self.critic_iter):
                self.D.zero_grad()

                # Clamp parameters to a range [-c, c], c=self.weight_cliping_limit
                for p in self.D.parameters():
                    p.data.clamp_(-self.weight_cliping_limit, self.weight_cliping_limit)
                #print(self.data)
                images = self.data.__next__()
                # Check for batch to have full batch_size
                if (images.size()[0] != self.batch_size):
                    continue

                z = torch.rand((self.batch_size, 100, 1, 1))

                if self.cuda:
                    images, z = Variable(images.cuda()), Variable(z.cuda())
                else:
                    images, z = Variable(images), Variable(z)


                # Train discriminator
                # WGAN - Training discriminator more iterations than generator
                # Train with real images
                d_loss_real = self.D(images)
                # print(np.shape(d_loss_real))
                d_loss_real = d_loss_real.mean(0).view(1)
                d_loss_real.backward(one)

                # Train with fake images
                if self.cuda:
                    z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda()
                else:
                    z = Variable(torch.randn(self.batch_size, 100, 1, 1))
                fake_images = self.G(z)
                d_loss_fake = self.D(fake_images)
                d_loss_fake = d_loss_fake.mean(0).view(1)
                d_loss_fake.backward(mone)

                d_loss = d_loss_fake - d_loss_real
                Wasserstein_D = d_loss_real - d_loss_fake
                self.d_optimizer.step()


            # Generator update
            for p in self.D.parameters():
                p.requires_grad = False  # to avoid computation

            self.G.zero_grad()

            # Train generator
            # Compute loss with fake images
            z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda()
            fake_images = self.G(z)
            g_loss = self.D(fake_images)
            g_loss = g_loss.mean().mean(0).view(1)
            g_loss.backward(one)
            g_cost = -g_loss
            self.g_optimizer.step()

            # Saving model and sampling images every 1000th generator iterations
            if (g_iter) % 500 == 0:
                self.save_model()
                #if not os.path.exists('training_result_images/'):
                 #   os.makedirs('training_result_images/')

                # Denormalize images and save them in grid 8x8
                #z = Variable(torch.randn(800, 100, 1, 1)).cuda(self.cuda_index)
                #samples = self.G(z)
                #samples = samples.mul(0.5).add(0.5)
                #samples = samples.data.cpu()[:64]
                #grid = utils.make_grid(samples)
                #utils.save_image(grid, self.IMG_OUT + 'img_generatori_iter_{}.png'.format(str(g_iter).zfill(3)))

                # Testing
                time = t.time() - self.t_begin
                print("Generator iter: {}".format(g_iter))
                print("Time {}".format(time))
                print("Generator Loss {}".format(g_loss))
                print("Descriminator Loss {}".format(d_loss))

        self.t_end = t.time()
        print('Time of training-{}'.format((self.t_end - self.t_begin)))
        #self.file.close()

        # Save the trained parameters
        self.save_model()

    def evaluate(self, D_model_path, G_model_path):
        self.load_model(D_model_path, G_model_path)
        z = Variable(torch.randn(self.batch_size, 100, 1, 1)).cuda()
        samples = self.G(z)
        samples = samples.mul(0.5).add(0.5)
        samples = samples.data.cpu()
        grid = utils.make_grid(samples)
        print("Grid of 8x8 images saved to 'dgan_model_image.png'.")
        utils.save_image(grid, self.IMG_OUT + 'dgan_model_image.png')

    def real_images(self, images, number_of_images):
        if (self.C == 3):
            return self.to_np(images.view(-1, self.C, 25, 25)[:self.number_of_images])
        else:
            return self.to_np(images.view(-1, 25, 25)[:self.number_of_images])

    def generate_img(self, z, number_of_images):
        samples = self.G(z).data.cpu().numpy()[:number_of_images]
        generated_images = []
        for sample in samples:
            if self.C == 3:
                generated_images.append(sample.reshape(self.C, 25, 25))
            else:
                generated_images.append(sample.reshape(25, 25))
        return generated_images

    def to_np(self, x):
        return x.data.cpu().numpy()

    def save_model(self):
        
        torch.save({
            'model_state_dict': self.G.state_dict(),
            'optimizer_state_dict': self.g_optimizer.state_dict()
        },self.MODEL_PATH + 'generator.pkl')

        torch.save({
            'model_state_dict': self.D.state_dict(),
            'optimizer_state_dict': self.d_optimizer.state_dict()
        },self.MODEL_PATH + 'discriminator.pkl')
        print('Models saved to ./generator.pkl & ./discriminator.pkl ')

    def load_model(self, D_model_filename, G_model_filename):
        D_model_path = self.MODEL_PATH + D_model_filename
        G_model_path = self.MODEL_PATH + G_model_filename
        D_checkpoint = torch.load(D_model_path)
        G_checkpoint = torch.load(G_model_path)
        
        # from collections import OrderedDict
        # new_G_checkpoint = OrderedDict()
        # for k, v in G_checkpoint():
        #     name = k[7:] # remove `module.`
        #     new_G_checkpoint[name] = v
        
        self.D.load_state_dict(D_checkpoint['model_state_dict'])
        self.G.load_state_dict(G_checkpoint['model_state_dict'])
        self.d_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])
        self.g_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])
        print('Generator model loaded from {}.'.format(G_model_path))
        print('Discriminator model loaded from {}-'.format(D_model_path))


    def get_infinite_batches(self, data_loader):
        while True:
            for i, (images) in enumerate(data_loader):
                yield images


    def generate_latent_walk(self, number):
        #if not os.path.exists('interpolated_images/'):
         #   os.makedirs('interpolated_images/')

        number_int = 10
        # interpolate between twe noise(z1, z2).
        z_intp = torch.FloatTensor(1, 100, 1, 1)
        z1 = torch.randn(1, 100, 1, 1)
        z2 = torch.randn(1, 100, 1, 1)
        if self.cuda:
            z_intp = z_intp.cuda()
            z1 = z1.cuda()
            z2 = z2.cuda()

        z_intp = Variable(z_intp)
        images = []
        alpha = 1.0 / float(number_int + 1)
        print(alpha)
        for i in range(1, number_int + 1):
            z_intp.data = z1*alpha + z2*(1.0 - alpha)
            alpha += alpha
            fake_im = self.G(z_intp)
            #fake_im = fake_im.mul(0.5).add(0.5) #denormalize
            images.append(fake_im.view(self.C,25,25).data.cpu())
        
        grid = utils.make_grid(images, nrow=number_int )
        utils.save_image(grid, self.INT_IMG + 'interpolated_{}.png'.format(str(number).zfill(3)))
        print("Saved interpolated images.")